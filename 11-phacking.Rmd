# False positives, p-hacking and multiple comparisons {#phacking}

<!---Paul -I have messed around with the permutations bit and am not 100% sure I have it right - in trying to explain it to others I realised I may have got the wrong end of the stick. 
But more generally, I think this chapter gets too complicated - particularly the material after the Bonferroni. Need to discuss how to handle it. It might be possible to simplify it further and just refer on to other sources. If there aren't good clear descriptions available, we could possibly could put it an appendix?
Another option would be to actually shift this stuff into Exercises at the end - i.e. flag up this is a more technical exercise, but point them to sources on BH and BY and get them to do some sums themselves? I think that way, people could treat it as optional for those who are more numerate.  
But I think the main thing is to get them to understand why p-hacking is a problem, and that you need to correct for multiple comparisons, without getting too bogged down in difficult stuff-->


```{r, echo=F,warning=F,message=F}
list_of_packages<-c("tidyverse","kableExtra","knitr","MASS")
new.packages <- list_of_packages[!(list_of_packages %in% installed.packages()[,"Package"])]
if(length(new.packages))install.packages(new.packages,dependencies = TRUE)

library(tidyverse)
library(kableExtra)
library(knitr)
library(MASS)
library(dplyr)
library(gtools) #for permutations
library(flextable)

```

Let us look again at the table of possible outcomes that can arise if we use p-values to decide if a result is statistically significant or not, shown here as \@ref(table:confusionMat2a). In this chapter, our focus will be on the top right hand corner - the False Positive. 

  
```{r confusionMat2a, echo=FALSE, message=FALSE, warnings=FALSE,results='asis',tab.caption="Possible outcomes of Null Hypothesis Statistical Test"}
options(kableExtra.html.bsTable = T)
mymat<-matrix(NA,nrow=2,ncol=3)

mymat[,1]<-c("Reject Null hypothesis","Do Not Reject Null Hypothesis")

mymat[,2]<-c("True Positive","False Negative \n (Type II error)")
mymat[,3]<-c("False Positive \n (Type I error)","True Negative")

mymat<-as.data.frame(mymat)
names(mymat)<-c("","Intervention effective","Intervention ineffective")

knitr::kable(mymat,escape = F, align = "c", booktabs = T) %>%
  kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F) %>%
  column_spec(1:1, bold = T) %>%
  column_spec(1:3,width = "9em") %>%
  add_header_above(c(" ","Ground truth" = 2))
```



### Type I error

A **Type I error** is a **false positive**, which occurs when the null hypothesis is rejected but a true effect is not actually present. One useful mnemonic for distinguishing Type I and Type II errors is to think of the sequence of events in the fable of the boy who cried wolf. This boy lived in a remote village and he used to have fun running out to the farmers in the fields and shouting "Wolf! Wolf!" and watching them all run inside. The farmers were making an (understandable) Type I error, of concluding there was a wolf when in fact there was none.  One day, however, a wolf turned up. The boy ran into the fields shouting "Wolf! Wolf!" but nobody took any notice of him, because they were used to his tricks. Sadly, all the sheep got eaten because the villagers had made a Type II error, assuming there was no wolf when there really was one.  
<!-- dorothy to try and get a picture of boy who cried wolf-->

The type I error rate is controlled by setting the significance criterion, $\alpha=0.05$, at 5%, or sometimes more conservatively 1% ($\alpha=0.01$). With $\alpha=0.05$, on average 1 in 20 statistically significant results will be false positives; when it is .01, then 1 in 100 will be false positives. The .05 cutoff is essentially arbitrary, but very widely adopted in the medical and social sciences literature, though debate continues as to whether a different convention should be adopted [@benjamin2018, @lakens2018].  

### Reasons for false positives  

There is one obvious reason why researchers get false positive results: chance. It follows from the definition given above, that if you adopt an alpha level of .05, you will wrongly conclude that your intervention is effective in 1 in 20 studies.  This is why we should never put strong confidence in, or introduce major policy changes on the basis of, a single study. The probability of getting a false positive once is .05, but if you replicate your finding, then it is far more likely that it is reliable - the probability of two false positives in a row is .05 * .05 = 0025, or 1 in 400.  

Unfortunately, though, false positives and non-replicable results are far more common in the literature than they should be if our scientific method was working properly. One reason, which will be covered in Chapter \@ref(pubbias), is publication bias. Quite simply, there is a huge bias in favour of publishing papers reporting statistically significant results, with null results getting quietly forgotten.  

A related reason is a selective focus on positive findings _within_ a study. Consider a study where the researcher measures children's skills on five different measures, comprehension, expression, mathematics, reading, and motor skills, but only one of them, comprehension, shows a statistically significant improvement (p < .05) after intervention that involves general "learning stimulation". It may seem reasonable to delete the other measures from the write-up, because they are uninteresting. Alternatively, the researcher may report all the results, but argue that there is a good reason why the intervention worked for this specific measure. Unfortunately, this would be badly misleading, because the statistical test needed for a study with five outcome measures is different from the one needed for a single measure. Failure to understand this point is widespread - insofar as it is recognised as a problem, it is thought of as a relatively minor issue.  Let's look at this example in more detail to illustrate why it is so serious.  

We assume in the example above that the researcher would have been equally likely to single any one of the five measures, provided it gave p < .05, regardless of which one it was, and tell a good story about why the intervention was specifically effective with that task. If that is so, then interpreting a p-value for each individual measure is inappropriate, because the implicit hypothesis that is being tested is "do _any_ of these measures improve after intervention?". The probability of a false positive for any specific measure is 1 in 20, but the probability that _at least_ one measure gives a false positive result is higher.  We can work it out as follows:

- With $\alpha$ set to. 05, the probability that any one measure gives a _nonsignificant_ result = .95.
- The probability that *all five measures* give a _nonsignificant_ result is found by multiplying the probabilities for the five tasks:  .95 * .95 * .95 * .95 * .95 = .77.
- So it follows that the probability that _at least one_ measure gives a significant result (p-value < .05) is 1-.77 = .23.

In other words, with five measures to consider, the probability that _at least one of them_ will give us p < .05 is not 1 in 20 but 1 in 4. The more measures there are, the worse it gets.  We will discuss solutions to this issue below (see Multiple hypothesis testing).  

Psychologists have developed a term to talk about the increase in false positives that arises when people pick out results on the basis that they have a p-value of .05, regardless of the context - *p-hacking* [@simonsohn2014a]. @bishop2016a coined the term *ghost variable* to refer to a variable that was measured but was then not reported because it did not give significant results - making it impossible to tell that p-hacking had occurred. Another term, *HARKing*, or "hypothesising after the results are known" [@kerr1998] is used to describe the tendency to rewrite not just the Discussion but also the Introduction of a paper to fit the result that has been obtained. Sadly, researchers are not just unaware that these behaviours can dramatically increase the rate of false positives in the literature - they are often encouraged by senior figures to adopt exactly these practices: a notorious example is that of @bem2004.  Perhaps the most common error is to regard a p-value as a kind of inherent property of a result that reflects its importance regardless of context. In fact, context is absolutely key: a single p-value below .05 has a very different meaning in a study where you only had one outcome measure than in a study where you tested several measures in case _any_ of them gave an interesting result. 

A related point is that you should *never generate and test a hypothesis using the same data*. After you have run your study, you may be enthusiastic about doing further research with a specific focus on the comprehension outcome measure. That's fine, and in a new study with specific predictions about comprehension you could adopt $\alpha$ of .05 without any corrections. Problems arise when you subtly change your hypothesis _after seeing the data_ from "Do _any_ of these N measures show interesting results?" to "Does comprehension improve after intervention?", and apply statistics as if the other measures had not been considered.  

In clinical trials research, potential for p-hacking is in theory limited by a requirement for  registration of trial protocols, which should ensure that a primary outcome measure is identified before the study is started (see Chapter \@ref(prereg)).  This has not yet become standard for behavioural interventions, and in practice, clinical trial researchers often deviate from the protocol after seeing the results [@goldacre2019]. It is important to be aware of the potential for a high rate of false positives when multiple outcomes are included. 

Does this mean that only a single outcome can be included? The answer is no: It might be the case that the researcher requires multiple outcomes to determine the effectiveness of an intervention, for example, a quantity of interest might not be able to be measured directly, so several proxy measures are recorded to provide a composite outcome measure. But in this case, it is important to plan in advance how to conduct the analysis to avoid an increased rate of false positives. There are statistical options that involve a more complex multivariate analysis, but this in turn may require a substantially greater sample size. There is a danger that if too many outcomes are included, it becomes harder to find an effect of interest, because in controlling for potential false positives, we risk a high rate of type II errors - see Chapter \@ref(power).
 
Selection of one measure from among many is just one form of p-hacking. Another common practice has been referred to as the "garden of forking paths": the practice of trying many different analyses, including making subgroups, changing how variables are categorised, excluding certain participants post hoc, or applying different statistical tests, in the quest for something significant. This has all the problems noted above in the case of selecting from multiple measures, except it is even harder to make adjustments to the analysis to take it into account because it is often unclear exactly how many different analyses could potentially have been run. With enough analyses it is almost always possible to find something that achieves the magic cutoff of p < .05. 

This [animation](https://figshare.com/articles/figure/The_Garden_of_Forking_Paths/2100379) tracks how the probability of a "significant" p-value below .05 increases as one does increasingly refined analyses of hypothetical data investigating a link between handedness and ADHD - with data split according to age, type of handedness test, gender, and whether the child's home is rural or urban. The probability of finding at least one significant result is tracked at the bottom of the display. For each binary split, the number of potential contrasts doubles, so at the end of the path there are 16 potential tests that could have been run, and the probability of at _at least one_ "significant" result in one combination of conditions is .56. The researcher may see that a p-value is below .05 and gleefully report that they have discovered an exciting association, but if they were looking for _any_ combination of values out of numerous possibilities, then the p-value is highly misleading - in particular, it is _not_ the case that there is only a 1 in 20 chance of obtaining a result this extreme.   

There are several ways we can defend ourselves against a proliferation of false positives that results if we are too flexible in data analysis:  

- Pre-registration of the study protocol, giving sufficient detail of measures and planned analyses to prevent flexible analysis - or at least make it clear when researchers have departed from the protocol. We cover pre-registration in more detail in Chapter \@ref(prereg).  

- Using statistical methods to correct for multiple testing. Specific methods are discussed below. Note, however, that this is only effective if we correct for all the possible analyses that were considered.  

- "Multiverse analysis": explicitly conducting all possible analyses to test how particular analytic decisions affected results.  This is beyond the scope of this book, as it is more commonly adopted in non-intervention research contexts [@steegen2016] when analysis of associations between variables is done with pre-existing data sets.  


## Adjusting statistics for multiple hypothesis testing  

As noted above, even when we have a well-designed and adequately powered study, if we collect multiple outcome variables, or if we are applying multiple tests to the same data, then we increase our chance of finding a false positive. Remember that if we set $\alpha$ to .05 and apply $k$ tests to our data, then the probability of finding at least one false positive is given by $1-(1-\alpha)^{k}$. This is officially known as the family-wise error rate (FWER).  

```{r familywise,echo=F, fig.cap="Plot of relationship between familywse error rate and number of statistical tests"}
k <- seq(1:100)
FWE <- (1-(1-0.05)^k)
fwe_dat<-data.frame(FWE=FWE,k=k)
ggplot(data=fwe_dat,aes(k,FWE,type="l"))+geom_line(size=1.2,colour="blue")+theme_bw()+geom_segment(aes(x=10,y=0,xend=10,yend=0.4),linetype="dashed",size=1.2)+geom_segment(aes(x=0,y=0.4,xend=10,yend=0.4),linetype="dashed",size=1.2)+xlab("Number of tests, k")+ylab("Familywise Error (FWE)")

```

Figure \@ref(fig:familywise) shows the relationship between the familywise error rate and the number of tests conducted on the data. Note that the left-most point of the blue line corresponds to the case when we have just one statistical test, and the probability of a false positive is .05, exactly where we would expect. The dotted line shows the case where we performed 10 tests (k = 10), increasing our chance of obtaining a false positive to approximately 40%.  Clearly, the more tests applied, the greater the increase in the chance of at least one false positive result. Although it may seem implausible that anyone would conduct 100 tests, the number of implicit tests can rapidly increase if sequential analytic decisions multiply, as shown in the Garden of Forking Paths example, where we subdivided the analysis 4 times, to give 2^4 = 16 possible ways of dividing up the data. 


There are many different ways to adjust for the multiple testing in practice. We shall discuss some of the most commonly applied.

### Bonferroni Correction
The Bonferroni correction is both the simplest and most popular adjustment for multiple testing. The test is described as "protecting the type I error rate", i.e. if you want to make a false positive error only 1 in 20 studies, the Bonferroni correction specifies a new $\alpha$ level that is adjusted for the number of tests. The Bonferroni correction is very easy to apply: you just divide your desired false positive rate by the number of tests conducted.  

For example, say we had some data and wanted to run multiple t-tests between two groups on 10 outcomes, and keep the false positive rate at 1 in 20. The Bonferroni correction would adjust the $\alpha$ level to be 0.05/10 = 0.005, which would indicate that the true false positive rate would be $1-(1-\alpha_{adjusted})^{n} = 1-(1-0.005)^{10} = 0.049$ which approximates our desired false positive rate. So we now treat any p-values greater than .005 as non-significant. This successfully controls our type I error rate at approximately 5%.  

It is not uncommon for researchers to report results both with and without Bonferroni correction - using phrases such as "the difference was significant but did not survive Bonferroni correction". This indicates misunderstanding of the statistics. The Bonferroni correction is not some kind of optional extra in an analysis that is just there to satisfy pernickety statisticians. If it is needed - as will be the case when multiple tests are conducted in the absence of clear a priori predictions -  then the raw uncorrected p-values are not meaningful, and should not be reported. 

The Bonferroni correction is widely used due to its simplicity but it can be over-conservative when our outcome measures are correlated. Nevertheless, for the kinds of scenario we might expect in an intevention context (where we may anticipate intercorrelations of around .4 to .6 for between measures) this does not have a very big effect. We illustrate this with some results of simulations, where we consider the case where we have either 2, 4, 6 or 8 outcome measures, which can range from being totally uncorrelated (r = 0), to strongly intercorrelated (r = .8). 

```{r simulated-corr,echo=F,include=T,tab.cap='Adjusted alpha-value to give 1 in 20 false positives for correlated outcome measures'}
mycorr <- c(0,.2,.4,.6,.8)
mynvar <- c(2,4,6,8)
maxn <- max(mynvar)
nsim <- 5000

# To run the simulation, which can take a few minutes, set runsim to 1.
# If runsim is zero, we read in results from file instead.
runsim <- 0

if(runsim==1){
#dataframe to hold p-values for each variable, comparing A and B
resultsdf <- data.frame(matrix(NA,nrow=nsim*length(mycorr),ncol=maxn+2+length(mynvar)))
colnames(resultsdf) <-c('run','corr',paste0('V',1:maxn),paste0('minp',mynvar) ) 

summarydf <- data.frame(matrix(NA,nrow=length(mycorr),ncol=1+length(mynvar)))
colnames(summarydf) <-c('corr',paste0('N',mynvar) )    
thisrow <- 0
for (s in 1:nsim){
  for (c in mycorr){
    thisrow <- thisrow+1
    resultsdf$run[thisrow] <- s
    resultsdf$corr[thisrow] <- c
      mysigma<-matrix(c,nrow=maxn,ncol=maxn)
      diag(mysigma)<-1
      A <- mvrnorm(n=100,mu=rep(0,maxn),Sigma=mysigma)
      B <- mvrnorm(n=100,mu=rep(0,maxn),Sigma=mysigma)
           for (v in 1:maxn) {
             tempt <- t.test(A[,v],B[,v])
             resultsdf[thisrow,(v+2)]<-tempt$p.value
           }
   }
}
# we now have resultsdf, which is a big dataframe with p-values from all the simulations, for max N variables at all corrs.

# for each row, we need the lowest p-value for a subset of vars
# so we take 1st 2, then 1st 4, then 1st 6 and finally all vars 

for (i in 1:nrow(resultsdf)){
  thiscol <- ncol(resultsdf)-length(mynvar)
  for (n in mynvar){
    thiscol<-thiscol+1 #column to write to 
    resultsdf[i,thiscol]<-min(resultsdf[i,(2+(1:n))])
  }
}

    

# we use this to populate summarydf, which holds critical alphas based on quantiles of p-values
# to check this, also create fpdf, which is similar but holds FP rate at .05
fpdf<-summarydf
thisrow <- 0
for (c in mycorr){
  thisrow<-thisrow+1
  colrange <- (ncol(resultsdf)-length(mynvar)+1):ncol(resultsdf)
  mybit <- resultsdf[resultsdf$corr==c,colrange] #select subset for this correlation
  thiscol<-0
  summarydf[thisrow,1]<-c
  fpdf[thisrow,1]<-c
  for (n in mynvar){
    thiscol<-thiscol+1
    plist <- mybit[,thiscol]
    fpdf[thisrow,(thiscol+1)]<-length(which(plist<.05))/length(plist)
    summarydf[thisrow,(thiscol+1)]<-quantile(plist,.05)
   
  }
}
write.csv(summarydf,'data\adj_alphas_corr_vars.csv',row.names=F)
write.csv(fpdf,'data\fprates_corr_vars.csv',row.names=F)
write.csv(resultsdf,'data\rawsims_corr_vars_10000runs.csv',row.names=F)
}

if(runsim==0){
  summarydf <- read.csv('data\adj_alphas_corr_vars.csv')
}

ft <- flextable(round(summarydf,3))
ft
```
Table \@ref(tab:simulated-corr) shows the significance threshold that we'd need to use to get a false positive rate of 1 in 20, if there are either 2, 4, 6 or 8 correlated outcome variables. The top row shows the case for uncorrelated variables, and the significance cutoffs are very close to those that you would get by just applying the Bonferroni correction. As the correlation (shown in the left-most column) increases, the adjusted significance level becomes somewhat less severe, but in practice we can see that for realistic scenarios in an intervention study, the difference is not large. Researchers could be justified in substituting the significance cutoffs from Table \@ref(tab:simulated-corr) for the Bonferroni correction, but it is unlikely to make a large difference to outcomes.  

We used the same simulated data to compute the false positive rate that would arise if the researcher just ignored the problems of running tests on multiple measures ; Table \@ref(tab:simulated-fp) reinforces the importance of adopting a statistical correction, rather than assuming it is acceptable to stick with the .05 significance level while running tests on many outcome measures. For instance, with 6 outcome measures that are intercorrelated at r = .2, the probability of a false positive result is .254. 

```{r simulated-fp,echo=F,include=T,tab.cap='False positive rate when .05 significance level used with multiple outcomes tested'}
ft <- flextable(round(fpdf,3))
ft
```

## CLASS EXERCISE  

This [web-based app](https://www.shinyapps.org/apps/p-hacker/) by @schonbrodt2016 allows you to see how easy it can be to get a "significant" result by using flexible analyses. The term DV refers to "dependent variable", i.e. an outcome measure.  
- Set the True Effect slider to its default position of zero, which means that the Null Hypothesis is true - there is no true effect.  
- Set the Number of DVs set to 2.  
- Press "Run new experiment".  
- The display of results selects the variable with the lowest p-value and highlights it in green. It also shows results for the average of the variables, as DV_all. 
- Try hitting Run new experiment 20 times, and count on your fingers how often you get a p-value less than .05. With only two DVs, things are not so bad - on average you'll get two such values in 20 runs (though you may get more or less than this - if you are really keen, you could keep going for 100 runs to get a more stable estimate of the false positives).  
- Now set the number of DVs to 10 and repeat the exercise. You will probably run out of fingers on one hand to count the number of significant p-values.  

Felix SchÃ¶nbrodt, who devised this website, allows you to go even further with p-hacking, showing how easy it is to nudge results into significance by using covariates (e.g. adjusting for age, SES, etc) or by excluding outliers. 

Note that if you enjoy playing with p-hacker, you can also use the app to improve your statistical intuitions about sample size and power. The app doesn't allow you to specify the case where there is only one outcome measure, which is what we really need in this case, so you have to just ignore all but one of the DVs. We suggest you just look at results for DV1. This time we'll assume you have a real effect. You can use the slider on True Effect to pick a value of Cohen's d, and you can also select the number of participants in each group. When you Run new experiment, you will find that it is difficult to get a p-value below .05 if the true effect is small and/or the sample size is small, whereas with a large sample and a big effect, a significant result is highly likely. 


<!-- possibly further exercise involving Bonferroni correction here-->