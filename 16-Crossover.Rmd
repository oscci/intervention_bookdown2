# Cross-over designs {#crossover}

```{r, message=F,warning=F,echo=FALSE}
list_of_packages<-c("tidyverse","kableExtra","knitr","ggpubr")
new.packages <- list_of_packages[!(list_of_packages %in% installed.packages()[,"Package"])]
if(length(new.packages))install.packages(new.packages,dependencies = TRUE)

library(tidyverse)
library(kableExtra)
library(knitr)
library(ggpubr)

```
<!-- for the wait-list control design, the Burgoyne et al study used previously in RCT chapter could be used to illustrate. 
this is a more standard cross-over design
https://eprints.whiterose.ac.uk/112815/1/Varley%20et%20al_Computerized%20therapy%20for%20AOS.pdf
-->
`r include_image("images/logo_alone_new.png")`



## A within-subjects approach to the RCT  
The standard two-arm RCT treats the intervention effect as a _between-subjects_ factor: i.e., the intervention and control groups include different people. In the cross-over design, we treat intervention as a _within-subjects_ factor, by comparing the same people in different phases of a study, during which they do or do not receive intervention. Because we have two groups who receive the period with and without intervention in counterbalanced order, we avoid the interpretive problems that arise with a simple pre-intervention vs post-intervention comparison (Chapter \@ref(prepost)).

The benefit of the crossover design is that a potentially more accurate evaluation of intervention comparison is achieved, as we compare individuals to themselves rather than controls who are different individuals. Another benefit is that power is higher than for a regular RCT: crossover designs typically require fewer individuals, as a separate control group is not necessary. In effect, with a cross-over design, we combine two sources of information about the efficacy of intervention: comparing individuals across the two phases (_within subjects_) and comparing the two groups in each phase (_between subjects_).

In drug trials, the typical crossover design is split into three phases: an initial phase where two groups are randomized (exactly the same as a parallel group design) to intervention and control [@sibbald1998]. Once the first phase has elapsed, there is a washout phase (phase 2) where no intervention is given; this is important to allow any intervention effect to be removed before the groups are switched. Then phase 3 is started with the intervention and control conditions switched. By the end of the study, both groups have received intervention. The assessment of the intervention looks at the differences between phases 1 and 3 by group. If treatment is effective, we should see no group difference but a significant phase difference.

```{r crossoverplot, echo=F, message=F, warning=F, fig.width=8,fig.height=4, fig.cap="Idealised results from a crossover RCT"}
#simulate some data based on a known structure
set.seed(123)
dat <- data.frame(x = runif(100, 1, 3.5), x2 = rnorm(100,0,0.25))
dat$intervention <- as.integer(dat$x >= 2.5)
dat$rev_intervention <- as.factor(as.integer(dat$x < 2.5))
dat$y <- 3 + 0.0001 * dat$x + 0.02 * dat$x2 + 4 * (dat$x >= 2.5) + rnorm(100,0,0.5)

for(i in 1:100)
{
if(dat$x[i]>2&dat$x[i]<2.5){dat$intervention[i]=2}
}
dat$intervention<-as.factor(dat$intervention)
newdat1<-dat[dat$intervention==0,]
newdat2<-dat[dat$intervention==1,]
newdat3<-dat[dat$intervention==2,]

mod1 <- lm(y~x,newdat1)
mod2 <- lm(y~x,newdat2)
 #plots for lmer 
 
 ggplot(dat[!dat$intervention==2,], aes(x = x, y = y)) + 
   theme_bw()+ theme(legend.position = "top",strip.text=element_text(),axis.text=element_text(),axis.title=element_text(face="bold"))+ annotate("text",x=2.25,y=8,label="Washout",size=5)+xlab("")+ylab("Outcome score") +
   annotate("text",x=1.5,y=8,label="Phase 1",size=5) +
   annotate("text",x=3,y=8,label="Phase 3",size=5)+
   annotate("text",x=1.5,y=7.5,label="Group 1 have intervention",size=4.5) +
   annotate("text",x=3,y=7.5,label="Group 2 have intervention",size=4.5)+
   annotate("rect",xmin=2,xmax=2.5,ymin=-Inf,ymax=Inf, alpha=0.1, fill="black")+
   geom_segment(aes(x=1,y=6,xend = 2, yend = 6),col="red",size=2) +
   geom_segment(aes(x=2.5,y=4,xend = 3.5, yend = 4),col="red",size=2)+
geom_segment(aes(x=2,y=6,xend = 2.5, yend = 4),col="red",linetype="dashed",size=2) +
   geom_segment(aes(x=2,y=4,xend = 2.5, yend = 6),col="blue",linetype="dashed",size=2)+
   geom_segment(aes(x=1,y=4,xend = 2, yend = 4),col="blue",size=2) +
   geom_segment(aes(x=2.5,y=6,xend = 3.5, yend = 6),col="blue",size=2)+ylim(3,8)+
   geom_segment(aes(x=2.6,y=4,xend = 2.6, yend = 6),col="red",size=2,linetype="dotted")+ylim(3,8)+
   annotate("text",x=2.75,y=5,label="effect\nsize",color="red",size=5) +
   annotate("text",x=1.5,y=6.2,label="Group 1",color="red",size=5) +
   annotate("text",x=1.5,y=4.2,label="Group 2",color="blue",size=5)+theme(axis.text = element_blank(),axis.ticks = element_blank())

#ggarrange(cross2,cross1,ncol=2,nrow=1)

```
The suitability of a cross-over design, and the anticipated results, depend crucially on the nature of the intervention.  A key question is whether or not the intervention is intended to have long-term effects that persist beyond the intervention phase. For most behavioural interventions, including those administered by speech-and-language therapists, educators and allied health professionals, the goal is to bring about long-term change. Exceptions would be communication aids such as auditory feedback masking, which decreases stuttering while the masker is switched on, but does not produce long-term change [@block1996]. In this regard, most behavioural interventions are unlike pharmaceutical trials, which often focus on the ability of specific drugs to provide symptomatic relief. This makes results from the crossover design difficult to interpret. 

### Delayed crossover design (Wait list controls)

The delayed crossover design or wait list control design is another type of _within subject_ design that is more commonly used in situations where the effect of an intervention is likely to persist. We start by taking baseline measures from both groups. The impact of intervention is measured in Group 1 relative to their pre-intervention score. For Group 2, intervention starts at the point when Group 1 stop the intervention. 


<!---We definitely need to cover this, as it is not uncommon esp in educational contexts. 
But thinking about this, I'm not sure how it differs from regular crossover apart from making it easier to recruit. And I guess it is possible to make it adaptive so the waitlist people only get intervention if analysis at end of phase 1 shows it is promising. 
Also wonder about the analysis: I googled and someone said this, 
"Have three timepoints for everyone, t0 at baseline, t1 when the control group switches to treatment, and t2 after about double that time. At t1 you have a between subject treatment comparison of treatment versus control adjusted for the t0 baseline covariate. You also have a within subject comparison within the control-treatment group. The right linear mixed model can combine those estimates under certain assumptions. And stronger assumptions can be made of course if blinding occurs and neither group knows who is on control or treatment to time t1." --> 

```{r waitlist-plot, echo=F, message=F, warning=F, fig.width=8,fig.height=4, fig.cap="Idealised results from a wait list design RCT"}



  ggplot(dat[!dat$intervention==2,], aes(x = x, y = y)) + 
   theme_bw()+ theme(legend.position = "top",strip.text=element_text(size=12),axis.text=element_text(size=12),axis.title=element_text(size=12,face="bold"))+ xlab("")+ylab("Outcome measure") +
   annotate("text",x=1.5,y=8,label="Period 1",size=5) +
   annotate("text",x=3,y=8,label="Period 2",size=5)+
   annotate("rect",xmin=2,xmax=2.5,ymin=-Inf,ymax=Inf, alpha=0.1, fill="black")+
   geom_segment(aes(x=1,y=6,xend = 2, yend = 6),col="red",size=1) +
   geom_segment(aes(x=2,y=6,xend = 2.5, yend = 6),col="red",linetype="dashed",size=1) +
   geom_segment(aes(x=2,y=4,xend = 2.5, yend = 6),col="blue",linetype="dashed",size=1)+
   geom_segment(aes(x=1,y=4,xend = 2, yend = 4),col="blue",size=1) +
   geom_segment(aes(x=2.5,y=6,xend = 3.5, yend = 6),col="purple",size=1)+ylim(3,8)+
   geom_segment(aes(x=1.8,y=4,xend = 1.8, yend = 6),col="black",size=1,linetype="dotted")+ylim(3,8)+
   annotate("text",x=1.65,y=5,label="effect\nsize",color="black",size=5) +
   annotate("text",x=1.3,y=6,label="Group 1\nreceives intervention",color="red",size=5) +
   annotate("text",x=1.3,y=4,label="Group 2\nhas no intervention",color="blue",size=5)+theme(axis.text = element_blank(),axis.ticks = element_blank())+annotate("text",x=3,y=5.5,label="Both groups receive\n intervention",color="purple",size=5)

```
In many respects, this design resembles a regular RCT, and has some of the same benefits, in terms of controlling for effects of practice, maturation and regression to the mean. Relative to a standard RCT, it has some advantages:  

- Group 2 serves as a replication sample. If benefits of intervention are seen in Group 1, then we should see similar effects in Group 2 by the end of the study.   
- As well as estimating immediate effects of intervention, Group 1 can provide an estimate of how far there is persistence of effects by comparing their score at the of intervention with the final, post-intervention phase.  
- An adaptive approach can be adopted, so that if no benefit of intervention is seen for Group 1 at the end of intervention, the study can be terminated.  
- This design may encourage participation by clients and those referring them, as all participants have access to a potentially beneficial intervention and serve as their own controls.  

There is, however, some debate around usefulness of wait-list designs in the psychological and behavioural interventions literature [@elliott2002; @cunningham2013; @furukawa2014]. In particular, @cunningham2013 presented results that showed this design can potentially, artificially inflate intervention effect estimates. When a participant knows that they have to wait, this can induce  a state of "resentful demoralisation", which can lead to poorer outcome - a kind of "nocebo" effect. Having said that, Cunningham et al noted that negative reactions to being on a wait list are likely to depend on the specific context. Studies showing "nocebo" effects have tended to focus on clinical psychology interventions, where distressed patients have been actively seeking help and may become disconsolate or irritated at having to wait. 

It is possible to have a wait-list design that avoids nocebo effects if a plausible active control is used. An example is provided by @varley2016, who used a self-administered computerised intervention with stroke patients who had apraxia of speech. The intervention group did training on speech production and perception for 6 weeks, whereas the active controls were given a sham intervention that involved visuospatial processing, and were told it was designed to improve their attention and memory. After the initial intervention period, there was a 4 week break, and then the two interventions were swapped over for another 6 weeks. The researchers noted, however, that this approach does involve some deception.  

@calder2021 used a delayed crossover design to study the effect of an intervention designed to improve use of part tense -ed endings in children with Developmental Language Disorder. As well as comparing two groups who received intervention at different time points, they also measured performance on a set of probe tasks. One of these assessed performance on past tense endings - the grammatical element that the training had focused on. Another assessed use of a different verb ending (3rd person singular), which was related but had not been trained. The third probe tested mastery of possessive 's.  This approach relates to methods used in single-case designs (Chapter \@ref(singlecase)), where additional evidence for intervention effects is obtained by illustrating that they are specific to the skills that are trained.

```{r calderfig, echo=F,fig.leg = "Mean % correct in delayed cross-over study by Calder et al, 2021 (data plotted from Table 2)."}

cdat <- read.csv("data/calder_xover.csv")
wantrows <- c(1,5,7,9,13,15)
cdat <-cdat[wantrows,]
#reorder cols to facilitate conversion
cdatwide <- cdat[,c(1,2,3,5,7,9,11,13,15,4,6,8,10,12,14,16)]
clong <- gather(cdatwide, phase, measurement, T1:T7, factor_key=TRUE)
clong$OutcomesGroup <-paste0(clong$Group,clong$Outcomes)
clong$OutcomesGroup<-as.factor(clong$OutcomesGroup)
clong$Outcomes <- as.factor(clong$Outcomes)
clong$phase <- as.factor(clong$phase)
levels(clong$phase)=1:7
clong$Group <- as.factor(clong$Group)

  
 #Try with base R!
plot(1, type = "n",                         # Remove all elements of plot
     xlab = "Phase", ylab = "% Correct",
     xlim = c(1, 7), ylim = c(0, 80))
mycols <- c('blue','red')
mylines <- c(1,2,3)
mypch<-c(2,0,19)
vpos<-c(75,10)
hpos <-c(4.5,6.5)
 for (g in 1:2){
    thisdat<-clong[clong$Group==g,]
    for (o in 1:3){
       plotdat<-thisdat[thisdat$Outcomes==levels(clong$Outcomes)[o],]
       plotdat<-plotdat[!is.na(plotdat$measurement),]
       lines(x=plotdat$phase,y=plotdat$measurement,col=mycols[g],lty=mylines[o],type='b',pch=mypch[o])
    }
    text(1.5,vpos[g],paste0("Group ",g,":\n baseline"),col=mycols[g])
    text(hpos[g],vpos[g],paste0("Group ",g,":\n post-intervention"),col=mycols[g])
 }
segments(1,68,2,68,col='blue',lwd=2)
segments(1,3,4,3,col="red",lwd=2)

# Add a legend
legend("topright", 
  legend = c("past -ed","3rd pers -s","possessive 's"), 
  pch = c(19,0,2), 
  inset = c(0.01, 0.01))
```

Results are shown in Figure \@ref(fig:calderfig). The filled circles show results on the trained past tense -ed items, illustrating nicely the way in which this design can combine sources of evidence that individually might not be compelling, but together create a coherent picture of an intervention effect. First, we have the improvement in group 1 when phases 4-5 (post-intervention) are compared with phases 1-2 (baseline). We will discuss in Chapter \@ref(singlecase) how we can use the additional evidence provided by outcomes for the two untrained constructions (3rd person singular and possessive 's), shown as unfilled figures, but for now we note that they do not show the same improvement as for the trained construction.  
Next, look at Group 2, in red. They provide two further pieces of evidence. First we can use them as a replication sample for Group 1, considering the change from phase 4-5 (baseline) to phase 6-7 (post-intervention): although the improvement on past-tense items is not as big as that seen in Group 1, it is evident by inspection, and again contrasts with a lack of improvement on untrained constructions. Finally, we can contrast the two groups at phases 4-5. This is more like the contrast performed in a standard RCT: a between-subjects contrast between treated vs untreated groups at the same time point. Here, the combined evidence from between- and within-subjects comparisons provides converging support for the effectiveness of the intervention. 


## Class exercise  
@calder2021 provides a nice example of how to design a study to provide a stringent test of intervention by combining within- and between-subjects sources of evidence. It should be remembered, though, that results don't always turn out as hoped. 
What could you conclude if:  
- There was no difference between Groups 1 and 2 on past tense items at phase 4, with neither group showing any improvement over baseline?   
- There was no difference between Groups 1 and 2 at phase 4, with both groups showing significant improvement over baseline?    
- Group 1 improved at phase 4, but Group 2 did not improve between phase 4 and phase 6?  

When planning a study of this kind, the natural tendency is to assume that everything will work out and show the intervention to be effective. It helps to design a strong study if one anticipates the possibility of messy or inconclusive results such as those described here, and considers whether it is possible to design a study to avoid them.  


